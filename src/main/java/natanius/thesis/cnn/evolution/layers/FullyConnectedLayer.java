package natanius.thesis.cnn.evolution.layers;

import static natanius.thesis.cnn.evolution.data.Constants.L2_REGULARIZATION_LAMBDA;
import static natanius.thesis.cnn.evolution.data.Constants.OUTPUT_CLASSES;
import static natanius.thesis.cnn.evolution.data.Constants.RANDOM;

import java.util.ArrayList;
import java.util.List;
import natanius.thesis.cnn.evolution.activation.Activation;
import natanius.thesis.cnn.evolution.activation.LeakyReLU;
import natanius.thesis.cnn.evolution.activation.Linear;
import natanius.thesis.cnn.evolution.activation.ReLU;
import natanius.thesis.cnn.evolution.activation.Sigmoid;

public class FullyConnectedLayer extends Layer {

    private final Activation activation;
    private final double[][] weights;
    private final double[] biases;
    private final int inLength;
    private final int outLength;
    private final double learningRate;
    private double l2Lambda = L2_REGULARIZATION_LAMBDA;
    private List<double[]> lastXBatch;
    private List<double[]> lastZBatch;

    public FullyConnectedLayer(Activation activation, int inLength, double learningRate) {
        this(activation, inLength, OUTPUT_CLASSES, learningRate);
    }

    public FullyConnectedLayer(Activation activation, int inLength, int outLength, double learningRate) {
        this.activation = activation;
        this.inLength = inLength;
        this.outLength = outLength;
        this.learningRate = learningRate;

        weights = new double[inLength][outLength];
        if (activation instanceof ReLU || activation instanceof LeakyReLU || activation instanceof Linear) {
            initWeightsHe();
        } else if (activation instanceof Sigmoid) {
            initWeightsXavier();
        } else {
            throw new IllegalArgumentException(
                "Unsupported activation function: " + activation.getClass().getSimpleName() +
                    ". Supported: ReLU, LeakyReLU, Sigmoid"
            );
        }

        biases = new double[outLength];
    }


    @Override
    public List<double[]> getOutputBatch(List<List<double[][]>> batchInput) {
        // –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤—Ö–æ–¥–∏ –∑ feature maps —É –≤–µ–∫—Ç–æ—Ä–∏
        List<double[]> batchVectors = new ArrayList<>();
        for (List<double[][]> input : batchInput) {
            batchVectors.add(matrixToVector(input));
        }

        // Forward pass –¥–ª—è –±–∞—Ç—á–∞
        List<double[]> output = fullyConnectedForwardPassBatch(batchVectors);

        // –ü–µ—Ä–µ–¥–∞—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É —à–∞—Ä—É, —è–∫—â–æ —î
        if (nextLayer != null) {
            List<List<double[][]>> outputAsFeatureMaps = new ArrayList<>();
            for (double[] vec : output) {
                List<double[][]> featureMap = vectorToMatrix(vec, 1, 1, vec.length);
                outputAsFeatureMaps.add(featureMap);
            }
            return nextLayer.getOutputBatch(outputAsFeatureMaps);
        }
        return output;
    }

    /**
     * –†–µ–∞–ª—ñ–∑—É—î –ø—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥ (forward propagation) —á–µ—Ä–µ–∑ –ø–æ–≤–Ω–æ–∑–≤'—è–∑–Ω–∏–π —à–∞—Ä.
     *
     * <p><b>–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω—ñ –ø–æ–∑–Ω–∞—á–µ–Ω–Ω—è:</b>
     * <ul>
     *   <li>input = a^(l-1) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É (–≤—Ö—ñ–¥ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —à–∞—Ä—É)</li>
     *   <li>weights = W^(l) ‚Äî –º–∞—Ç—Ä–∏—Ü—è –≤–∞–≥ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —à–∞—Ä—É</li>
     *   <li>biases = b^(l) ‚Äî –≤–µ–∫—Ç–æ—Ä –∑–º—ñ—â–µ–Ω—å –ø–æ—Ç–æ—á–Ω–æ–≥–æ —à–∞—Ä—É</li>
     *   <li>z = z^(l) ‚Äî –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞ (–ø–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–∞—Ü—ñ—î—é)</li>
     *   <li>a = a^(l) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø—ñ—Å–ª—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó</li>
     * </ul>
     *
     * <p><b>–ö—Ä–æ–∫ 1:</b> –û–±—á–∏—Å–ª–µ–Ω–Ω—è –∑–≤–∞–∂–µ–Ω–æ—ó —Å—É–º–∏:
     * <pre>
     *   z^(l) = W^(l) ¬∑ a^(l-1) + b^(l)
     * </pre>
     * –¥–µ ¬∑ –æ–∑–Ω–∞—á–∞—î –º–∞—Ç—Ä–∏—á–Ω–µ –º–Ω–æ–∂–µ–Ω–Ω—è (—É –∫–æ–¥—ñ: weights[i][j] * input[i]).
     *
     * <p><b>–ö—Ä–æ–∫ 2:</b> –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó:
     * <pre>
     *   a^(l) = f^(l)(z^(l))
     * </pre>
     * –¥–µ f^(l) ‚Äî —Ñ—É–Ω–∫—Ü—ñ—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó —à–∞—Ä—É (ReLU, Sigmoid —Ç–æ—â–æ).
     *
     * <p><b>–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è:</b> –ü—ñ–¥ —á–∞—Å –æ–±—á–∏—Å–ª–µ–Ω–Ω—è z –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—å—Å—è –Ω—É–ª—å–æ–≤—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ input[i],
     * —â–æ –æ—Å–æ–±–ª–∏–≤–æ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—ñ—Å–ª—è ReLU –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –∞–±–æ pooling –æ–ø–µ—Ä–∞—Ü—ñ–π.
     *
     * <p><b>–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–ª—è backpropagation:</b>
     * <ul>
     *   <li>lastX = a^(l-1) ‚Äî –≤—Ö—ñ–¥–Ω—ñ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó</li>
     *   <li>lastZ = z^(l) ‚Äî –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞ –ø–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–∞—Ü—ñ—î—é</li>
     * </ul>
     *
     * @param batchInputs —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä—ñ–≤ a^(l-1) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—É –≤ –±–∞—Ç—á—ñ
     * @return —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä—ñ–≤ a^(l) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ—Ç–æ—á–Ω–æ–≥–æ —à–∞—Ä—É –ø—ñ—Å–ª—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è f^(l)
     */
    public List<double[]> fullyConnectedForwardPassBatch(List<double[]> batchInputs) {
        List<double[]> batchOutputs = new ArrayList<>();
        lastXBatch = new ArrayList<>();
        lastZBatch = new ArrayList<>();

        for (double[] input : batchInputs) {
            validateInput(input);

            double[] z = biases.clone();

            for (int i = 0; i < inLength; i++) {
                double aPrevI = input[i];
                if (aPrevI != 0.0) {
                    double[] wRow = weights[i];
                    for (int j = 0; j < outLength; j++) {
                        z[j] += wRow[j] * aPrevI;
                    }
                }
            }

            // –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è a^(l-1) –¥–ª—è backpropagation
            lastXBatch.add(input.clone());
            lastZBatch.add(z.clone());

            double[] a = applyActivation(z);
            batchOutputs.add(a);
        }

        return batchOutputs;
    }

    private void validateInput(double[] input) {
        if (input.length != inLength) {
            throw new IllegalArgumentException(
                "Expected input length " + inLength + ", got " + input.length
            );
        }
    }

    /**
     * –ó–∞—Å—Ç–æ—Å–æ–≤—É—î —Ñ—É–Ω–∫—Ü—ñ—é –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó f^(l) –¥–æ –∫–æ–∂–Ω–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞ –≤–µ–∫—Ç–æ—Ä–∞ z^(l).
     *
     * @param z –≤–µ–∫—Ç–æ—Ä z^(l) ‚Äî –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞
     * @return –≤–µ–∫—Ç–æ—Ä a^(l) = f^(l)(z^(l)) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø—ñ—Å–ª—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó
     */
    private double[] applyActivation(double[] z) {
        double[] a = new double[outLength];
        for (int j = 0; j < outLength; j++) {
            a[j] = activation.forward(z[j]);
        }
        return a;
    }


    /**
     * –†–µ–∞–ª—ñ–∑—É—î –∞–ª–≥–æ—Ä–∏—Ç–º –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –ø–æ—à–∏—Ä–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏ (backpropagation) —á–µ—Ä–µ–∑ –ø–æ–≤–Ω–æ–∑–≤'—è–∑–Ω–∏–π —à–∞—Ä.
     * –ü—Ä–æ—Ü–µ—Å —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ —á–æ—Ç–∏—Ä—å–æ—Ö –µ—Ç–∞–ø—ñ–≤: –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ—ó –ø–æ—Ö–∏–±–∫–∏, –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞
     * –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É, –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥ —Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–º—ñ—â–µ–Ω—å.
     *
     * <p><b>–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω—ñ –ø–æ–∑–Ω–∞—á–µ–Ω–Ω—è:</b>
     * <ul>
     *   <li>dLda = ‚àÇL/‚àÇa^(l) ‚Äî –≥—Ä–∞–¥—ñ—î–Ω—Ç –≤—Ç—Ä–∞—Ç –≤—ñ–¥–Ω–æ—Å–Ω–æ –≤–∏—Ö–æ–¥—É —à–∞—Ä—É (–≤—Ö—ñ–¥–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä)</li>
     *   <li>delta = Œ¥^(l) ‚Äî –ª–æ–∫–∞–ª—å–Ω–∞ –ø–æ—Ö–∏–±–∫–∞ —à–∞—Ä—É</li>
     *   <li>dLdX = ‚àÇL/‚àÇa^(l-1) ‚Äî –≥—Ä–∞–¥—ñ—î–Ω—Ç –≤—Ç—Ä–∞—Ç –≤—ñ–¥–Ω–æ—Å–Ω–æ –≤—Ö–æ–¥—É —à–∞—Ä—É</li>
     *   <li>lastZ = z^(l) ‚Äî –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞ –ø–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–∞—Ü—ñ—î—é (–∑–±–µ—Ä–µ–∂–µ–Ω–∞ –∑ forward pass)</li>
     *   <li>lastX = a^(l-1) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É (–∑–±–µ—Ä–µ–∂–µ–Ω—ñ –∑ forward pass)</li>
     *   <li>weights = W^(l) ‚Äî –º–∞—Ç—Ä–∏—Ü—è –≤–∞–≥</li>
     *   <li>biases = b^(l) ‚Äî –≤–µ–∫—Ç–æ—Ä –∑–º—ñ—â–µ–Ω—å</li>
     * </ul>
     *
     * <p><b>–ï–¢–ê–ü 1: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ—ó –ø–æ—Ö–∏–±–∫–∏</b>
     * <pre>
     *   Œ¥^(l) = ‚àÇL/‚àÇa^(l) ‚äô f'(z^(l))
     * </pre>
     * –¥–µ ‚äô ‚Äî –ø–æ–µ–ª–µ–º–µ–Ω—Ç–Ω–µ –º–Ω–æ–∂–µ–Ω–Ω—è (Hadamard product), f' ‚Äî –ø–æ—Ö—ñ–¥–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó.
     * <p>
     *
     * <p><b>–ï–¢–ê–ü 2: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É</b>
     * <pre>
     *   ‚àÇL/‚àÇa^(l-1) = (W^(l))^T ¬∑ Œ¥^(l)
     * </pre>
     * –¶–µ–π –≥—Ä–∞–¥—ñ—î–Ω—Ç –ø–µ—Ä–µ–¥–∞—î—Ç—å—Å—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–º—É —à–∞—Ä—É –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è backpropagation.
     * <p>
     *
     * <p><b>–ï–¢–ê–ü 3: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤</b>
     * <pre>
     *   ‚àÇL/‚àÇW^(l)_ij = a^(l-1)_i ¬∑ Œ¥^(l)_j
     *   ‚àÇL/‚àÇb^(l)_j = Œ¥^(l)_j
     * </pre>
     * <p>
     *
     * <p><b>–ï–¢–ê–ü 4: –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–µ—Ç–æ–¥–æ–º –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫—É</b>
     * <pre>
     *   W^(l) := W^(l) - Œ∑ ¬∑ ‚àÇL/‚àÇW^(l)
     *   b^(l) := b^(l) - Œ∑ ¬∑ ‚àÇL/‚àÇb^(l)
     * </pre>
     * –¥–µ Œ∑ ‚Äî —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (learning rate).
     *
     * <p><b>–í–ê–ñ–õ–ò–í–û:</b> –ì—Ä–∞–¥—ñ—î–Ω—Ç dLdX –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è –î–û –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏
     * —Å—Ç–∞—Ä—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤. –¶–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –∫–æ—Ä–µ–∫—Ç–Ω–æ—Å—Ç—ñ backpropagation —á–µ—Ä–µ–∑ –≤–µ—Å—å –ª–∞–Ω—Ü—é–≥ —à–∞—Ä—ñ–≤.
     *
     * @param dLdaBatch –≥—Ä–∞–¥—ñ—î–Ω—Ç —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –≤—ñ–¥–Ω–æ—Å–Ω–æ –≤–∏—Ö–æ–¥—É —à–∞—Ä—É (‚àÇL/‚àÇa^(l))
     */
    @Override
    public void backPropagationBatch(List<double[]> dLdaBatch) {
        int batchSize = dLdaBatch.size();

        // –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–∫—É–º—É–ª—è—Ç–æ—Ä—ñ–≤ –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
        double[][] weightsDeltaSum = new double[inLength][outLength];
        double[] biasesDeltaSum = new double[outLength];

        List<double[]> dLdaPrevBatch = new ArrayList<>();

        // –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–µ–Ω –ø—Ä–∏–∫–ª–∞–¥ —É –±–∞—Ç—á—ñ
        for (int b = 0; b < batchSize; b++) {
            double[] dLda = dLdaBatch.get(b);  // –≥—Ä–∞–¥—ñ—î–Ω—Ç –≤–∏—Ö–æ–¥—É
            double[] lastX = lastXBatch.get(b);  // –≤—Ö—ñ–¥
            double[] lastZ = lastZBatch.get(b);  // z –ø–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–∞—Ü—ñ—î—é

            // –ï–¢–ê–ü 1: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ—ó –ø–æ—Ö–∏–±–∫–∏ 
            // Œ¥^(l) = ‚àÇL/‚àÇa^(l) ‚äô f'(z^(l))
            double[] delta = new double[outLength];
            for (int j = 0; j < outLength; j++) {
                delta[j] = dLda[j] * activation.backward(lastZ[j]);
            }

            // –ï–¢–ê–ü 2: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É 
            // ‚àÇL/‚àÇa^(l-1) = (W^(l))^T ¬∑ Œ¥^(l)
            double[] dLdaPrev = new double[inLength];
            for (int i = 0; i < inLength; i++) {
                double sum = 0.0;
                double[] wRow = weights[i];
                for (int j = 0; j < outLength; j++) {
                    sum += wRow[j] * delta[j];
                }
                dLdaPrev[i] = sum;
            }

            // –ï–¢–ê–ü 3: –ê–∫—É–º—É–ª—è—Ü—ñ—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ 
            // ‚àÇL/‚àÇW^(l)_ij = a^(l-1)_i ¬∑ Œ¥^(l)_j
            for (int i = 0; i < inLength; i++) {
                double aPrevI = lastX[i];
                for (int j = 0; j < outLength; j++) {
                    double dLdWij = aPrevI * delta[j];
                    weightsDeltaSum[i][j] += dLdWij;
                }
            }

            // ‚àÇL/‚àÇb^(l)_j = Œ¥^(l)_j
            for (int j = 0; j < outLength; j++) {
                biasesDeltaSum[j] += delta[j];
            }

            dLdaPrevBatch.add(dLdaPrev);
        }

        // –ï–¢–ê–ü 4: –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (—É—Å–µ—Ä–µ–¥–Ω–µ–Ω—ñ –ø–æ –±–∞—Ç—á—É) 
        for (int i = 0; i < inLength; i++) {
            for (int j = 0; j < outLength; j++) {
                double grad = weightsDeltaSum[i][j] / batchSize;
                grad += l2Lambda * weights[i][j];  // L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
                weights[i][j] -= learningRate * grad;
            }
        }

        for (int j = 0; j < outLength; j++) {
            biases[j] -= learningRate * (biasesDeltaSum[j] / batchSize);
        }

        if (previousLayer != null) {
            previousLayer.backPropagationBatch(dLdaPrevBatch);
        }
    }


    private void initWeightsHe() {
        double std = Math.sqrt(2.0 / inLength);
        for (int i = 0; i < inLength; i++) {
            for (int j = 0; j < outLength; j++) {
                weights[i][j] = RANDOM.nextGaussian() * std;
            }
        }
    }

    private void initWeightsXavier() {
        double limit = Math.sqrt(6.0 / (inLength + outLength));
        for (int i = 0; i < inLength; i++) {
            for (int j = 0; j < outLength; j++) {
                weights[i][j] = (RANDOM.nextDouble() * 2 - 1) * limit;
            }
        }
    }


    @Override
    public int getOutputLength() {
        return outLength;
    }

    @Override
    public int getOutputRows() {
        return 1;
    }

    @Override
    public int getOutputCols() {
        return outLength;
    }

    @Override
    public int getOutputElements() {
        return outLength;
    }

    @Override
    public int getParameterCount() {
        return inLength * outLength + outLength;
    }

    @Override
    public String toString() {
        return String.format("üîó FULLY CONNECTED | Inputs: %d ‚Üí Outputs: %d | Parameters: %d",
            inLength, outLength, getParameterCount());
    }
}
