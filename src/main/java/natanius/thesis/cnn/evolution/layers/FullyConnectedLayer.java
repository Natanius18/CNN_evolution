package natanius.thesis.cnn.evolution.layers;

import static natanius.thesis.cnn.evolution.data.Constants.L2_REGULARIZATION_LAMBDA;
import static natanius.thesis.cnn.evolution.data.Constants.OUTPUT_CLASSES;
import static natanius.thesis.cnn.evolution.data.Constants.RANDOM;

import java.util.ArrayList;
import java.util.List;
import natanius.thesis.cnn.evolution.activation.Activation;
import natanius.thesis.cnn.evolution.activation.LeakyReLU;
import natanius.thesis.cnn.evolution.activation.Linear;
import natanius.thesis.cnn.evolution.activation.ReLU;
import natanius.thesis.cnn.evolution.activation.Sigmoid;

public class FullyConnectedLayer extends Layer {

    private final Activation activation;
    private final double[][] weights;
    private final double[] biases;
    private final int inLength;
    private final int outLength;
    private final double learningRate;
    private double l2Lambda = L2_REGULARIZATION_LAMBDA;

    // Batch storage –¥–ª—è backpropagation
    private List<double[]> lastXBatch;
    private List<double[]> lastZBatch;

    public FullyConnectedLayer(Activation activation, int inLength, double learningRate) {
        this(activation, inLength, OUTPUT_CLASSES, learningRate);
    }

    public FullyConnectedLayer(Activation activation, int inLength, int outLength, double learningRate) {
        this.activation = activation;
        this.inLength = inLength;
        this.outLength = outLength;
        this.learningRate = learningRate;

        weights = new double[inLength][outLength];
        if (activation instanceof ReLU || activation instanceof LeakyReLU || activation instanceof Linear) {
            initWeightsHe();
        } else if (activation instanceof Sigmoid) {
            initWeightsXavier();
        } else {
            throw new IllegalArgumentException(
                "Unsupported activation function: " + activation.getClass().getSimpleName() +
                    ". Supported: ReLU, LeakyReLU, Sigmoid"
            );
        }

        biases = new double[outLength];
    }

    // ========== BATCH FORWARD PASS ==========

    @Override
    public List<double[]> getOutputBatch(List<List<double[][]>> batchInput) {
        // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Ö–æ–¥—ã –∏–∑ feature maps –≤ –≤–µ–∫—Ç–æ—Ä—ã
        List<double[]> batchVectors = new ArrayList<>();
        for (List<double[][]> input : batchInput) {
            batchVectors.add(matrixToVector(input));
        }

        // Forward pass –¥–ª—è –±–∞—Ç—á–∞
        List<double[]> output = fullyConnectedForwardPassBatch(batchVectors);

        // –ü–µ—Ä–µ–¥–∞—ë–º —Å–ª–µ–¥—É—é—â–µ–º—É —Å–ª–æ—é, –µ—Å–ª–∏ –µ—Å—Ç—å
        if (nextLayer != null) {
            // ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û: nextLayer.getOutputBatch(null);
            // ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û: –ø–µ—Ä–µ–¥–∞—ë–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ –±–∞—Ç—á List<List<double[][]>>

            // FC –æ–±—ã—á–Ω–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø–µ—Ä–µ–¥ –≤—ã—Ö–æ–¥–æ–º, –Ω–æ –µ—Å–ª–∏ –µ—Å—Ç—å –µ—â—ë —Å–ª–æ–∏,
            // –Ω—É–∂–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å output –æ–±—Ä–∞—Ç–Ω–æ –≤ List<List<double[][]>>
            List<List<double[][]>> outputAsFeatureMaps = new ArrayList<>();
            for (double[] vec : output) {
                List<double[][]> featureMap = vectorToMatrix(vec, 1, 1, vec.length);
                outputAsFeatureMaps.add(featureMap);
            }
            return nextLayer.getOutputBatch(outputAsFeatureMaps);
        }
        return output;
    }

    /**
     * Forward pass –¥–ª—è –±–∞—Ç—á–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤
     * –ö–∞–∂–¥—ã–π –≤–µ–∫—Ç–æ—Ä –≤ —Å–ø–∏—Å–∫–µ ‚Äî —ç—Ç–æ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∏–∑ –±–∞—Ç—á–∞
     */
    public List<double[]> fullyConnectedForwardPassBatch(List<double[]> batchInputs) {
        List<double[]> batchOutputs = new ArrayList<>();
        lastXBatch = new ArrayList<>();
        lastZBatch = new ArrayList<>();

        for (double[] input : batchInputs) {
            // –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–∞
            if (input.length != inLength) {
                throw new IllegalArgumentException(
                    "Expected input length " + inLength + ", got " + input.length
                );
            }

            // Forward –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            double[] z = biases.clone();

            for (int i = 0; i < inLength; i++) {
                double aPrevI = input[i];
                if (aPrevI != 0.0) {
                    double[] wRow = weights[i];
                    for (int j = 0; j < outLength; j++) {
                        z[j] += wRow[j] * aPrevI;
                    }
                }
            }

            // –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è backprop
            lastXBatch.add(input.clone());
            lastZBatch.add(z.clone());

            // –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏—é
            double[] a = new double[outLength];
            for (int j = 0; j < outLength; j++) {
                a[j] = activation.forward(z[j]);
            }
            batchOutputs.add(a);
        }

        return batchOutputs;
    }

    // ========== BATCH BACKPROPAGATION ==========

    @Override
    public void backPropagationBatch(List<double[]> dLdOBatch) {
        int batchSize = dLdOBatch.size();

        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        double[][] weightsDeltaSum = new double[inLength][outLength];
        double[] biasesDeltaSum = new double[outLength];

        List<double[]> dLdOPrevBatch = new ArrayList<>();

        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –≤ –±–∞—Ç—á–µ
        for (int b = 0; b < batchSize; b++) {
            double[] dLda = dLdOBatch.get(b);  // –≥—Ä–∞–¥–∏–µ–Ω—Ç –≤—ã—Ö–æ–¥–∞
            double[] lastX = lastXBatch.get(b);  // –≤—Ö–æ–¥
            double[] lastZ = lastZBatch.get(b);  // z –ø–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π

            // === –≠–¢–ê–ü 1: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–∏ ===
            // Œ¥^(l) = ‚àÇL/‚àÇa^(l) ‚äô f'(z^(l))
            double[] delta = new double[outLength];
            for (int j = 0; j < outLength; j++) {
                delta[j] = dLda[j] * activation.backward(lastZ[j]);
            }

            // === –≠–¢–ê–ü 2: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è ===
            // ‚àÇL/‚àÇa^(l-1) = (W^(l))^T ¬∑ Œ¥^(l)
            double[] dLdaPrev = new double[inLength];
            for (int i = 0; i < inLength; i++) {
                double sum = 0.0;
                double[] wRow = weights[i];
                for (int j = 0; j < outLength; j++) {
                    sum += wRow[j] * delta[j];
                }
                dLdaPrev[i] = sum;
            }

            // === –≠–¢–ê–ü 3: –ê–∫–∫—É–º—É–ª—è—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ===
            // ‚àÇL/‚àÇW^(l)_ij = a^(l-1)_i ¬∑ Œ¥^(l)_j
            for (int i = 0; i < inLength; i++) {
                double aPrevI = lastX[i];
                for (int j = 0; j < outLength; j++) {
                    double dLdWij = aPrevI * delta[j];
                    weightsDeltaSum[i][j] += dLdWij;
                }
            }

            // ‚àÇL/‚àÇb^(l)_j = Œ¥^(l)_j
            for (int j = 0; j < outLength; j++) {
                biasesDeltaSum[j] += delta[j];
            }

            dLdOPrevBatch.add(dLdaPrev);
        }

        // === –≠–¢–ê–ü 4: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–µ –ø–æ –±–∞—Ç—á—É) ===
        for (int i = 0; i < inLength; i++) {
            for (int j = 0; j < outLength; j++) {
                double grad = weightsDeltaSum[i][j] / batchSize;
                grad += l2Lambda * weights[i][j];  // L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
                weights[i][j] -= learningRate * grad;
            }
        }

        for (int j = 0; j < outLength; j++) {
            biases[j] -= learningRate * (biasesDeltaSum[j] / batchSize);
        }

        // –ü–µ—Ä–µ–¥–∞—ë–º –±–∞—Ç—á –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É —Å–ª–æ—é
        if (previousLayer != null) {
            previousLayer.backPropagationBatch(dLdOPrevBatch);
        }
    }

    // ========== WEIGHT INITIALIZATION ==========

    private void initWeightsHe() {
        double std = Math.sqrt(2.0 / inLength);
        for (int i = 0; i < inLength; i++) {
            for (int j = 0; j < outLength; j++) {
                weights[i][j] = RANDOM.nextGaussian() * std;
            }
        }
    }

    private void initWeightsXavier() {
        double limit = Math.sqrt(6.0 / (inLength + outLength));
        for (int i = 0; i < inLength; i++) {
            for (int j = 0; j < outLength; j++) {
                weights[i][j] = (RANDOM.nextDouble() * 2 - 1) * limit;
            }
        }
    }

    // ========== METADATA ==========

    @Override
    public int getOutputLength() {
        return outLength;
    }

    @Override
    public int getOutputRows() {
        return 1;
    }

    @Override
    public int getOutputCols() {
        return outLength;
    }

    @Override
    public int getOutputElements() {
        return outLength;
    }

    @Override
    public int getParameterCount() {
        return inLength * outLength + outLength;
    }

    @Override
    public String toString() {
        return String.format("üîó FULLY CONNECTED | Inputs: %d ‚Üí Outputs: %d | Parameters: %d",
            inLength, outLength, getParameterCount());
    }
}
