package natanius.thesis.cnn.evolution.layers;

import static natanius.thesis.cnn.evolution.data.Constants.OUTPUT_CLASSES;
import static natanius.thesis.cnn.evolution.data.Constants.RANDOM;

import java.util.List;
import lombok.Getter;
import natanius.thesis.cnn.evolution.activation.Activation;
import natanius.thesis.cnn.evolution.activation.LeakyReLU;
import natanius.thesis.cnn.evolution.activation.Linear;
import natanius.thesis.cnn.evolution.activation.ReLU;
import natanius.thesis.cnn.evolution.activation.Sigmoid;

@Getter // todo remove
public class FullyConnectedLayer extends Layer {

    private final Activation activation;

    private final double[][] weights;
    private final double[] biases;

    private final int inLength;
    private final int outLength;
    private final double learningRate;

    private double[] lastZ;
    private double[] lastX;

    public FullyConnectedLayer(Activation activation, int inLength, double learningRate) {
        this(activation, inLength, OUTPUT_CLASSES, learningRate);
    }

    public FullyConnectedLayer(Activation activation, int inLength, int outLength, double learningRate) {
        this.activation = activation;
        this.inLength = inLength;
        this.outLength = outLength;
        this.learningRate = learningRate;

        weights = new double[inLength][outLength];
        if (activation instanceof ReLU || activation instanceof LeakyReLU || activation instanceof Linear) {
            initWeightsHe();
        } else if (activation instanceof Sigmoid) {
            initWeightsXavier();
        } else {
            throw new IllegalArgumentException(
                "Unsupported activation function: " + activation.getClass().getSimpleName() +
                    ". Supported: ReLU, LeakyReLU, Sigmoid"
            );
        }

        biases = new double[outLength];

    }


    /**
     * –†–µ–∞–ª—ñ–∑—É—î –ø—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥ (forward propagation) —á–µ—Ä–µ–∑ –ø–æ–≤–Ω–æ–∑–≤'—è–∑–Ω–∏–π —à–∞—Ä.
     *
     * <p><b>–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω—ñ –ø–æ–∑–Ω–∞—á–µ–Ω–Ω—è:</b>
     * <ul>
     *   <li>input = a^(l-1) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É (–≤—Ö—ñ–¥ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —à–∞—Ä—É)</li>
     *   <li>weights = W^(l) ‚Äî –º–∞—Ç—Ä–∏—Ü—è –≤–∞–≥ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —à–∞—Ä—É</li>
     *   <li>biases = b^(l) ‚Äî –≤–µ–∫—Ç–æ—Ä –∑–º—ñ—â–µ–Ω—å –ø–æ—Ç–æ—á–Ω–æ–≥–æ —à–∞—Ä—É</li>
     *   <li>z = z^(l) ‚Äî –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞ (–ø–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–∞—Ü—ñ—î—é)</li>
     *   <li>a = a^(l) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø—ñ—Å–ª—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó</li>
     * </ul>
     *
     * <p><b>–ö—Ä–æ–∫ 1:</b> –û–±—á–∏—Å–ª–µ–Ω–Ω—è –∑–≤–∞–∂–µ–Ω–æ—ó —Å—É–º–∏:
     * <pre>
     *   z^(l) = W^(l) ¬∑ a^(l-1) + b^(l)
     * </pre>
     * –¥–µ ¬∑ –æ–∑–Ω–∞—á–∞—î –º–∞—Ç—Ä–∏—á–Ω–µ –º–Ω–æ–∂–µ–Ω–Ω—è (—É –∫–æ–¥—ñ: weights[i][j] * input[i]).
     *
     * <p><b>–ö—Ä–æ–∫ 2:</b> –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó:
     * <pre>
     *   a^(l) = f^(l)(z^(l))
     * </pre>
     * –¥–µ f^(l) ‚Äî —Ñ—É–Ω–∫—Ü—ñ—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó —à–∞—Ä—É (ReLU, Sigmoid —Ç–æ—â–æ).
     *
     * <p><b>–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è:</b> –ü—ñ–¥ —á–∞—Å –æ–±—á–∏—Å–ª–µ–Ω–Ω—è z –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—å—Å—è –Ω—É–ª—å–æ–≤—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ input[i],
     * —â–æ –æ—Å–æ–±–ª–∏–≤–æ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—ñ—Å–ª—è ReLU –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –∞–±–æ pooling –æ–ø–µ—Ä–∞—Ü—ñ–π.
     *
     * <p><b>–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–ª—è backpropagation:</b>
     * <ul>
     *   <li>lastX = a^(l-1) ‚Äî –≤—Ö—ñ–¥–Ω—ñ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó</li>
     *   <li>lastZ = z^(l) ‚Äî –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞ –ø–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–∞—Ü—ñ—î—é</li>
     * </ul>
     *
     * @param input –≤–µ–∫—Ç–æ—Ä a^(l-1) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É
     * @return –≤–µ–∫—Ç–æ—Ä a^(l) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ—Ç–æ—á–Ω–æ–≥–æ —à–∞—Ä—É –ø—ñ—Å–ª—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è f^(l)
     */
    public double[] fullyConnectedForwardPass(double[] input) {
        validateInput(input);

        // –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è a^(l-1) –¥–ª—è backpropagation
        lastX = input.clone();

        // –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è z^(l) = b^(l)
        double[] z = biases.clone();

        // –û–±—á–∏—Å–ª–µ–Ω–Ω—è z^(l) = W^(l) ¬∑ a^(l-1) + b^(l)
        for (int i = 0; i < inLength; i++) {
            double aPrevI = input[i];  // a^(l-1)_i

            if (aPrevI != 0.0) {
                double[] wRow = weights[i];

                for (int j = 0; j < outLength; j++) {
                    z[j] += wRow[j] * aPrevI;
                }
            }
        }
        lastZ = z;

        return applyActivation(z);
    }

    private void validateInput(double[] input) {
        if (input.length != inLength) {
            throw new IllegalArgumentException("Expected input length " + inLength + ", got " + input.length);
        }
    }

    /**
     * –ó–∞—Å—Ç–æ—Å–æ–≤—É—î —Ñ—É–Ω–∫—Ü—ñ—é –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó f^(l) –¥–æ –∫–æ–∂–Ω–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞ –≤–µ–∫—Ç–æ—Ä–∞ z^(l).
     *
     * @param z –≤–µ–∫—Ç–æ—Ä z^(l) ‚Äî –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞
     * @return –≤–µ–∫—Ç–æ—Ä a^(l) = f^(l)(z^(l)) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø—ñ—Å–ª—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó
     */
    private double[] applyActivation(double[] z) {
        double[] a = new double[outLength];
        for (int j = 0; j < outLength; j++) {
            a[j] = activation.forward(z[j]);
        }
        return a;
    }

    @Override
    public double[] getOutput(List<double[][]> input) {
        double[] vector = matrixToVector(input);
        return getOutput(vector);
    }

    @Override
    public double[] getOutput(double[] input) {
        double[] forwardPass = fullyConnectedForwardPass(input);

        return nextLayer == null ? forwardPass : nextLayer.getOutput(forwardPass);
    }

    /**
     * –†–µ–∞–ª—ñ–∑—É—î –∞–ª–≥–æ—Ä–∏—Ç–º –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –ø–æ—à–∏—Ä–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏ (backpropagation) —á–µ—Ä–µ–∑ –ø–æ–≤–Ω–æ–∑–≤'—è–∑–Ω–∏–π —à–∞—Ä.
     * –ü—Ä–æ—Ü–µ—Å —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ —á–æ—Ç–∏—Ä—å–æ—Ö –µ—Ç–∞–ø—ñ–≤: –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ—ó –ø–æ—Ö–∏–±–∫–∏, –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞
     * –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É, –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥ —Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–º—ñ—â–µ–Ω—å.
     *
     * <p><b>–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω—ñ –ø–æ–∑–Ω–∞—á–µ–Ω–Ω—è:</b>
     * <ul>
     *   <li>dLda = ‚àÇL/‚àÇa^(l) ‚Äî –≥—Ä–∞–¥—ñ—î–Ω—Ç –≤—Ç—Ä–∞—Ç –≤—ñ–¥–Ω–æ—Å–Ω–æ –≤–∏—Ö–æ–¥—É —à–∞—Ä—É (–≤—Ö—ñ–¥–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä)</li>
     *   <li>delta = Œ¥^(l) ‚Äî –ª–æ–∫–∞–ª—å–Ω–∞ –ø–æ—Ö–∏–±–∫–∞ —à–∞—Ä—É</li>
     *   <li>dLdX = ‚àÇL/‚àÇa^(l-1) ‚Äî –≥—Ä–∞–¥—ñ—î–Ω—Ç –≤—Ç—Ä–∞—Ç –≤—ñ–¥–Ω–æ—Å–Ω–æ –≤—Ö–æ–¥—É —à–∞—Ä—É</li>
     *   <li>lastZ = z^(l) ‚Äî –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞ –ø–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–∞—Ü—ñ—î—é (–∑–±–µ—Ä–µ–∂–µ–Ω–∞ –∑ forward pass)</li>
     *   <li>lastX = a^(l-1) ‚Äî –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É (–∑–±–µ—Ä–µ–∂–µ–Ω—ñ –∑ forward pass)</li>
     *   <li>weights = W^(l) ‚Äî –º–∞—Ç—Ä–∏—Ü—è –≤–∞–≥</li>
     *   <li>biases = b^(l) ‚Äî –≤–µ–∫—Ç–æ—Ä –∑–º—ñ—â–µ–Ω—å</li>
     * </ul>
     *
     * <p><b>–ï–¢–ê–ü 1: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ—ó –ø–æ—Ö–∏–±–∫–∏</b>
     * <pre>
     *   Œ¥^(l) = ‚àÇL/‚àÇa^(l) ‚äô f'(z^(l))
     * </pre>
     * –¥–µ ‚äô ‚Äî –ø–æ–µ–ª–µ–º–µ–Ω—Ç–Ω–µ –º–Ω–æ–∂–µ–Ω–Ω—è (Hadamard product), f' ‚Äî –ø–æ—Ö—ñ–¥–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó.
     * <p>
     *
     * <p><b>–ï–¢–ê–ü 2: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É</b>
     * <pre>
     *   ‚àÇL/‚àÇa^(l-1) = (W^(l))^T ¬∑ Œ¥^(l)
     * </pre>
     * –¶–µ–π –≥—Ä–∞–¥—ñ—î–Ω—Ç –ø–µ—Ä–µ–¥–∞—î—Ç—å—Å—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–º—É —à–∞—Ä—É –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è backpropagation.
     * <p>
     *
     * <p><b>–ï–¢–ê–ü 3: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤</b>
     * <pre>
     *   ‚àÇL/‚àÇW^(l)_ij = a^(l-1)_i ¬∑ Œ¥^(l)_j
     *   ‚àÇL/‚àÇb^(l)_j = Œ¥^(l)_j
     * </pre>
     * <p>
     *
     * <p><b>–ï–¢–ê–ü 4: –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–µ—Ç–æ–¥–æ–º –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫—É</b>
     * <pre>
     *   W^(l) := W^(l) - Œ∑ ¬∑ ‚àÇL/‚àÇW^(l)
     *   b^(l) := b^(l) - Œ∑ ¬∑ ‚àÇL/‚àÇb^(l)
     * </pre>
     * –¥–µ Œ∑ ‚Äî —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (learning rate).
     *
     * <p><b>–í–ê–ñ–õ–ò–í–û:</b> –ì—Ä–∞–¥—ñ—î–Ω—Ç dLdX –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è –î–û –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏
     * —Å—Ç–∞—Ä—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤. –¶–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –∫–æ—Ä–µ–∫—Ç–Ω–æ—Å—Ç—ñ backpropagation —á–µ—Ä–µ–∑ –≤–µ—Å—å –ª–∞–Ω—Ü—é–≥ —à–∞—Ä—ñ–≤.
     *
     * @param dLda –≥—Ä–∞–¥—ñ—î–Ω—Ç —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –≤—ñ–¥–Ω–æ—Å–Ω–æ –≤–∏—Ö–æ–¥—É —à–∞—Ä—É (‚àÇL/‚àÇa^(l))
     */
    @Override
    public void backPropagation(double[] dLda) {
        // –ï–¢–ê–ü 1: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ—ó –ø–æ—Ö–∏–±–∫–∏ Œ¥^(l)
        // Œ¥^(l) = ‚àÇL/‚àÇa^(l) ‚äô f'(z^(l))
        double[] delta = new double[outLength];
        for (int j = 0; j < outLength; j++) {
            delta[j] = dLda[j] * activation.backward(lastZ[j]);
        }

        // –ï–¢–ê–ü 2: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —à–∞—Ä—É
        // ‚àÇL/‚àÇa^(l-1) = (W^(l))^T ¬∑ Œ¥^(l)
        // –í–ê–ñ–õ–ò–í–û: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –°–¢–ê–†–Ü –∑–Ω–∞—á–µ–Ω–Ω—è –≤–∞–≥ (–¥–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è)
        double[] dLdaPrev = new double[inLength];
        for (int i = 0; i < inLength; i++) {
            double sum = 0.0;
            double[] wRow = weights[i];
            for (int j = 0; j < outLength; j++) {
                sum += wRow[j] * delta[j];
            }
            dLdaPrev[i] = sum;
        }

        // –ï–¢–ê–ü 3: –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥
        // W^(l)_ij := W^(l)_ij - Œ∑ ¬∑ ‚àÇL/‚àÇW^(l)_ij
        // –¥–µ ‚àÇL/‚àÇW^(l)_ij = a^(l-1)_i ¬∑ Œ¥^(l)_j
        for (int i = 0; i < inLength; i++) {
            double aPrevI = lastX[i];
            double[] wRow = weights[i];
            for (int j = 0; j < outLength; j++) {
                double dLdWij = aPrevI * delta[j];
                wRow[j] -= learningRate * dLdWij;
            }
        }

        // –ï–¢–ê–ü 4: –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–º—ñ—â–µ–Ω—å
        // b^(l)_j := b^(l)_j - Œ∑ ¬∑ ‚àÇL/‚àÇb^(l)_j
        // –¥–µ ‚àÇL/‚àÇb^(l)_j = Œ¥^(l)_j
        for (int j = 0; j < outLength; j++) {
            biases[j] -= learningRate * delta[j];
        }

        // –†–µ–∫—É—Ä—Å–∏–≤–Ω–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏ –Ω–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —à–∞—Ä
        if (previousLayer != null) {
            previousLayer.backPropagation(dLdaPrev);
        }
    }


    @Override
    public void backPropagation(List<double[][]> dLdO) {
        double[] vector = matrixToVector(dLdO);
        backPropagation(vector);
    }

    @Override
    public int getOutputLength() {
        return outLength;
    }

    @Override
    public int getOutputRows() {
        return 0;
    }

    @Override
    public int getOutputCols() {
        return 0;
    }

    @Override
    public int getOutputElements() {
        return outLength;
    }

    @Override
    public int getParameterCount() {
        return inLength * outLength + outLength;
    }

    @Override
    public String toString() {
        return String.format("üîó FULLY CONNECTED | Inputs: %d ‚Üí Outputs: %d | Parameters: %d",
            inLength, outLength, getParameterCount());
    }


    private void initWeightsHe() {
        double std = Math.sqrt(2.0 / inLength);
        for (int i = 0; i < inLength; i++) {
            for (int j = 0; j < outLength; j++) {
                weights[i][j] = RANDOM.nextGaussian() * std;
            }
        }
    }

    private void initWeightsXavier() {
        double limit = Math.sqrt(6.0 / (inLength + outLength));
        for (int i = 0; i < inLength; i++) {
            for (int j = 0; j < outLength; j++) {
                weights[i][j] = (RANDOM.nextDouble() * 2 - 1) * limit;
            }
        }
    }

}
